{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 — Test StudentModel (tiny fine-tune + EM eval)\n",
        "\n",
        "This notebook smoke-tests the `StudentModel` wrapper:\n",
        "- loads processed GSM8K JSON\n",
        "- builds tokenized datasets\n",
        "- runs a short training loop (few steps)\n",
        "- evaluates Exact Match on extracted final answer\n",
        "\n",
        "Tip: start with a small subset to validate the end-to-end flow, then scale.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/marcobonvissuto/Desktop/Università/Magistrale/Secondo Anno/NLP/project/gsm8k-distillation/data/processed/gsm8k_train_processed.json True\n",
            "/Users/marcobonvissuto/Desktop/Università/Magistrale/Secondo Anno/NLP/project/gsm8k-distillation/data/processed/gsm8k_test_processed.json True\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "REPO_ROOT = Path('.').resolve().parents[1]\n",
        "src_path = (REPO_ROOT / 'src').as_posix()\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "from models.student_model import StudentModel, StudentModelConfig\n",
        "\n",
        "PROCESSED_DIR = REPO_ROOT / 'gsm8k-distillation' / 'data' / 'processed'\n",
        "train_path = PROCESSED_DIR / 'gsm8k_train_processed.json'\n",
        "test_path  = PROCESSED_DIR / 'gsm8k_test_processed.json'\n",
        "print(train_path, train_path.exists())\n",
        "print(test_path,  test_path.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: 7473 test: 1319\n",
            "sample keys: dict_keys(['question', 'reasoning', 'answer', 'split', 'index'])\n"
          ]
        }
      ],
      "source": [
        "# Load processed examples\n",
        "train_examples = StudentModel.load_processed_json(train_path)\n",
        "test_examples  = StudentModel.load_processed_json(test_path)\n",
        "print('train:', len(train_examples), 'test:', len(test_examples))\n",
        "print('sample keys:', train_examples[0].keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Configure student\n",
        "cfg = StudentModelConfig(\n",
        "    model_name='google/flan-t5-small',\n",
        "    max_source_length=256,\n",
        "    max_target_length=256,\n",
        ")\n",
        "student = StudentModel(cfg)\n",
        "print('Device:', student.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "222d4868f7304688b3dc4f42155da034",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/256 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9f1e4108d814e8697d8815d4be549d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/64 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 256\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 64\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Build datasets: choose supervision='cot' (distillation) or 'answer' (baseline)\n",
        "supervision = 'cot'\n",
        "\n",
        "# Smoke-test with small subsets\n",
        "train_tok = student.build_hf_dataset(train_examples, supervision=supervision, limit=256, shuffle=True)\n",
        "eval_tok  = student.build_hf_dataset(test_examples,  supervision=supervision, limit=64, shuffle=False)\n",
        "\n",
        "print(train_tok)\n",
        "print(eval_tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/marcobonvissuto/Desktop/Università/Magistrale/Secondo Anno/NLP/project/src/models/student_model.py:237: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12/32 35:41 < 1:11:23, 0.00 it/s, Epoch 0.34/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Train briefly (adjust for your GPU/CPU)\n",
        "trainer = student.train(\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=eval_tok,\n",
        "    output_dir='outputs/student_smoke',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    logging_steps=10,\n",
        "    eval_steps=50,\n",
        "    save_steps=200,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Exact Match on raw examples (extracting last number from generation)\n",
        "metrics = student.evaluate_exact_match(test_examples, limit=50, num_beams=1)\n",
        "print('EM:', metrics['exact_match'], 'n:', metrics['n'])\n",
        "\n",
        "print('\\nTop-5 debug rows (question, generation, pred, gold):')\n",
        "for q, gen, pred, gold in metrics['pred_examples']:\n",
        "    print('\\nQ:', q[:120])\n",
        "    print('GEN:', gen[:220])\n",
        "    print('PRED:', pred, 'GOLD:', gold)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "student.save('outputs/student_smoke/final')\n",
        "print('Saved to outputs/student_smoke/final')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
